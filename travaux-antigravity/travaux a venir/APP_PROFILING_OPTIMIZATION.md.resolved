# ðŸ”¬ Guide de Profiling et Optimisation - Gestio V4

**Philosophie** : Mesurer d'abord, optimiser ensuite. Pas de suppositions, que des donnÃ©es.

---

## ðŸŽ¯ Objectifs

1. **Identifier** les vrais bottlenecks (fonctions lentes, requÃªtes inefficaces)
2. **Mesurer** l'impact de chaque optimisation
3. **Optimiser** le code applicatif, pas l'environnement
4. **Documenter** les gains pour prioriser les efforts

**RÃ©sultat attendu** : App 5-10x plus rapide, mÃªme sur WSL lent

---

## ðŸ“Š Phase 1 : Instrumentation (Mesurer tout)

### 1.1 DÃ©corateur de profiling

CrÃ©ez `app/shared/profiling.py` :

```python
"""Outils de profiling et mesure de performances"""
import time
import functools
import logging
from typing import Callable, Any
from contextlib import contextmanager

logger = logging.getLogger(__name__)

# Seuils d'alerte (en secondes)
SLOW_FUNCTION_THRESHOLD = 0.1  # 100ms
VERY_SLOW_FUNCTION_THRESHOLD = 0.5  # 500ms

class PerformanceMonitor:
    """Collecte et affiche les statistiques de performance"""
    
    def __init__(self):
        self.timings = {}  # {function_name: [durations]}
        self.db_queries = {}  # {query_type: count}
    
    def record_timing(self, name: str, duration: float):
        """Enregistre le temps d'exÃ©cution d'une fonction"""
        if name not in self.timings:
            self.timings[name] = []
        self.timings[name].append(duration)
    
    def record_query(self, query_type: str):
        """Compte les requÃªtes DB par type"""
        self.db_queries[query_type] = self.db_queries.get(query_type, 0) + 1
    
    def get_report(self) -> str:
        """GÃ©nÃ¨re un rapport de performance"""
        lines = ["ðŸ“Š Performance Report", "=" * 60]
        
        # Top fonctions lentes
        if self.timings:
            lines.append("\nðŸŒ Slowest Functions:")
            sorted_funcs = sorted(
                [(name, sum(times)/len(times), len(times)) 
                 for name, times in self.timings.items()],
                key=lambda x: x[1],
                reverse=True
            )[:10]
            
            for name, avg_time, count in sorted_funcs:
                lines.append(f"  {name:40} {avg_time*1000:6.1f}ms (x{count})")
        
        # RequÃªtes DB
        if self.db_queries:
            lines.append("\nðŸ“ Database Queries:")
            total = sum(self.db_queries.values())
            for query_type, count in sorted(self.db_queries.items(), key=lambda x: x[1], reverse=True):
                lines.append(f"  {query_type:40} {count:4} ({count/total*100:.1f}%)")
        
        return "\n".join(lines)

# Instance globale
monitor = PerformanceMonitor()

def profile(func: Callable) -> Callable:
    """DÃ©corateur pour mesurer automatiquement le temps d'exÃ©cution"""
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        start = time.perf_counter()
        result = func(*args, **kwargs)
        duration = time.perf_counter() - start
        
        # Enregistrer
        monitor.record_timing(func.__name__, duration)
        
        # Alerter si lent
        if duration > VERY_SLOW_FUNCTION_THRESHOLD:
            logger.warning(f"ðŸŒ VERY SLOW: {func.__name__} took {duration:.3f}s")
        elif duration > SLOW_FUNCTION_THRESHOLD:
            logger.info(f"âš ï¸  SLOW: {func.__name__} took {duration:.3f}s")
        
        return result
    return wrapper

@contextmanager
def measure_block(name: str):
    """Context manager pour mesurer un bloc de code"""
    start = time.perf_counter()
    try:
        yield
    finally:
        duration = time.perf_counter() - start
        monitor.record_timing(name, duration)
        if duration > SLOW_FUNCTION_THRESHOLD:
            logger.info(f"â±ï¸  {name}: {duration:.3f}s")

def track_query(query_type: str):
    """DÃ©corateur pour tracker les requÃªtes DB"""
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            monitor.record_query(query_type)
            return func(*args, **kwargs)
        return wrapper
    return decorator
```

### 1.2 IntÃ©grer dans le code existant

**Exemple** : `domains/transactions/services.py`

```python
from shared.profiling import profile, measure_block, track_query, monitor

class TransactionService:
    
    @profile
    @track_query("get_all_transactions")
    def get_all_transactions(self, limit: int = 100):
        """RÃ©cupÃ¨re toutes les transactions"""
        with measure_block("db_query_transactions"):
            query = """
                SELECT t.*, c.name as category_name, a.name as account_name
                FROM transactions t
                LEFT JOIN categories c ON t.category_id = c.id
                LEFT JOIN accounts a ON t.account_id = a.id
                ORDER BY t.date DESC
                LIMIT ?
            """
            results = self.db.execute(query, (limit,))
        
        with measure_block("pandas_dataframe_creation"):
            df = pd.DataFrame(results)
        
        return df
    
    @profile
    def compute_statistics(self, df):
        """Calcule les statistiques"""
        with measure_block("groupby_category"):
            by_category = df.groupby('category_name').agg({
                'amount': ['sum', 'count', 'mean']
            })
        
        with measure_block("groupby_month"):
            df['month'] = pd.to_datetime(df['date']).dt.to_period('M')
            by_month = df.groupby('month')['amount'].sum()
        
        return by_category, by_month
```

### 1.3 Afficher le rapport dans l'UI

**Dans** [main.py](file:///c:/Users/djabi/Desktop/linux/main.py) :

```python
from shared.profiling import monitor
import streamlit as st

def main():
    # ... votre code existant ...
    
    # Mode debug en sidebar
    with st.sidebar:
        if st.checkbox("ðŸ”¬ Show Performance Report", value=False):
            st.code(monitor.get_report(), language="text")
            
            if st.button("ðŸ”„ Reset Stats"):
                monitor.timings.clear()
                monitor.db_queries.clear()
                st.rerun()
```

---

## ðŸ” Phase 2 : Identifier les ProblÃ¨mes

### 2.1 Checklist des problÃ¨mes courants

**RequÃªtes DB** :
- [ ] N+1 queries (boucle + query Ã  chaque itÃ©ration)
- [ ] SELECT * au lieu de colonnes spÃ©cifiques
- [ ] Pas d'indices sur colonnes filtrÃ©es
- [ ] RequÃªtes sans LIMIT
- [ ] Pas de cache pour donnÃ©es statiques

**Pandas** :
- [ ] Conversion SQL â†’ DataFrame trop lente
- [ ] `.iterrows()` au lieu de vectorisation
- [ ] `.apply()` avec lambda sur gros DataFrame
- [ ] Pas de chunking pour gros datasets

**Streamlit** :
- [ ] Pas de `@st.cache_data` sur fonctions lourdes
- [ ] Re-calcul Ã  chaque interaction
- [ ] Chargement de tout au dÃ©marrage
- [ ] Pas de lazy loading

**Python** :
- [ ] Boucles imbriquÃ©es O(nÂ²)
- [ ] Listes au lieu de sets pour lookup
- [ ] Pas de comprÃ©hension de liste
- [ ] JSON parse/stringify rÃ©pÃ©tÃ©

### 2.2 Outils de diagnostic

#### SQLite Query Analyzer

```python
# Dans votre Database class
class Database:
    def __init__(self):
        # ... existing code ...
        
        # Activer logging SQL
        self.conn.set_trace_callback(self._log_query)
    
    def _log_query(self, query: str):
        """Log toutes les requÃªtes SQL"""
        logger.debug(f"SQL: {query}")
        
        # DÃ©tecter requÃªtes potentiellement lentes
        if "SELECT *" in query.upper():
            logger.warning(f"âš ï¸  Using SELECT * : {query[:50]}...")
        
        if "LIMIT" not in query.upper() and "SELECT" in query.upper():
            logger.warning(f"âš ï¸  No LIMIT clause: {query[:50]}...")
```

#### Pandas Memory Profiler

```python
from shared.profiling import profile

@profile
def analyze_dataframe_memory(df: pd.DataFrame, name: str = "DataFrame"):
    """Analyse l'utilisation mÃ©moire d'un DataFrame"""
    memory_mb = df.memory_usage(deep=True).sum() / 1024 / 1024
    
    if memory_mb > 10:  # Alert si > 10 MB
        logger.warning(f"ðŸ“Š Large {name}: {memory_mb:.2f} MB, {len(df)} rows")
        
        # Top colonnes consommatrices
        mem_by_col = df.memory_usage(deep=True).sort_values(ascending=False)
        logger.info(f"Top memory columns:\n{mem_by_col.head()}")
    
    return memory_mb
```

---

## âš¡ Phase 3 : Optimisations CiblÃ©es

### 3.1 Optimisation RequÃªtes DB

#### Avant (N+1 query)
```python
# âŒ 1 query pour liste + N queries pour dÃ©tails
transactions = db.execute("SELECT * FROM transactions")
for t in transactions:
    category = db.execute("SELECT name FROM categories WHERE id = ?", (t.category_id,))
    account = db.execute("SELECT name FROM accounts WHERE id = ?", (t.account_id,))
```

#### AprÃ¨s (1 query avec JOINs)
```python
# âœ… 1 seule query
query = """
    SELECT 
        t.id, t.date, t.amount, t.description,
        c.name as category_name,
        a.name as account_name
    FROM transactions t
    LEFT JOIN categories c ON t.category_id = c.id
    LEFT JOIN accounts a ON t.account_id = a.id
    ORDER BY t.date DESC
    LIMIT 1000
"""
transactions = db.execute(query)
```

**Gain attendu** : 50-100x plus rapide

#### CrÃ©er les bons indices

```sql
-- Analyser l'utilisation des colonnes
-- Dans SQLite, utilisez EXPLAIN QUERY PLAN

EXPLAIN QUERY PLAN
SELECT * FROM transactions WHERE date > '2024-01-01';

-- CrÃ©er indices si nÃ©cessaire
CREATE INDEX IF NOT EXISTS idx_transactions_date ON transactions(date);
CREATE INDEX IF NOT EXISTS idx_transactions_category ON transactions(category_id);
CREATE INDEX IF NOT EXISTS idx_transactions_account ON transactions(account_id);

-- Index composite pour filtres multiples
CREATE INDEX IF NOT EXISTS idx_trans_date_cat 
ON transactions(date, category_id) 
WHERE date IS NOT NULL;
```

### 3.2 Optimisation Pandas

#### Avant (lent)
```python
# âŒ Iteration row-by-row
for index, row in df.iterrows():
    df.at[index, 'total'] = row['price'] * row['quantity']

# âŒ Apply avec lambda
df['total'] = df.apply(lambda row: row['price'] * row['quantity'], axis=1)
```

#### AprÃ¨s (vectorisÃ©)
```python
# âœ… OpÃ©ration vectorielle
df['total'] = df['price'] * df['quantity']

# âœ… OpÃ©rations groupÃ©es
summary = df.groupby('category').agg({
    'amount': ['sum', 'mean', 'count'],
    'date': ['min', 'max']
})
```

**Gain attendu** : 10-100x plus rapide

#### Chunking pour gros datasets

```python
@profile
def process_large_dataset(file_path: str, chunk_size: int = 10000):
    """Traite un gros fichier par morceaux"""
    results = []
    
    for chunk in pd.read_csv(file_path, chunksize=chunk_size):
        # Traiter le chunk
        processed = process_chunk(chunk)
        results.append(processed)
    
    return pd.concat(results)
```

### 3.3 Optimisation Streamlit

#### Cache intelligent

```python
import streamlit as st

# Cache donnÃ©es statiques (catÃ©gories, comptes)
@st.cache_data(ttl=3600)  # 1 heure
def load_categories():
    return db.get_all_categories()

# Cache avec invalidation manuelle
@st.cache_data
def load_transactions(start_date, end_date, _cache_key):
    return db.get_transactions_between(start_date, end_date)

# Usage
cache_key = st.session_state.get('data_version', 0)
transactions = load_transactions(start, end, cache_key)

# Invalider cache si donnÃ©es modifiÃ©es
if st.button("Add Transaction"):
    add_transaction(data)
    st.session_state.data_version = cache_key + 1
    st.rerun()
```

#### Lazy loading et pagination

```python
# Au lieu de tout charger
if 'current_page' not in st.session_state:
    st.session_state.current_page = 0

page = st.session_state.current_page
page_size = 50
offset = page * page_size

# Charger seulement la page actuelle
transactions = db.get_transactions(limit=page_size, offset=offset)

# Navigation
col1, col2, col3 = st.columns([1,2,1])
with col1:
    if st.button("â† Previous") and page > 0:
        st.session_state.current_page -= 1
        st.rerun()
with col3:
    if st.button("Next â†’"):
        st.session_state.current_page += 1
        st.rerun()
```

### 3.4 Optimisations Python

#### Utiliser les bonnes structures

```python
# âŒ Lent - Liste pour lookup
categories = [...]
if cat_id in [c['id'] for c in categories]:  # O(n)
    ...

# âœ… Rapide - Set ou dict
category_ids = {c['id'] for c in categories}
if cat_id in category_ids:  # O(1)
    ...

# Ou mieux : dict pour accÃ¨s direct
categories_by_id = {c['id']: c for c in categories}
category = categories_by_id.get(cat_id)
```

#### ComprÃ©hensions au lieu de boucles

```python
# âŒ Lent
result = []
for item in items:
    if item.amount > 100:
        result.append(item.description.upper())

# âœ… Rapide
result = [item.description.upper() for item in items if item.amount > 100]
```

---

## ðŸ“ˆ Phase 4 : Benchmarks et Validation

### 4.1 Mesurer avant/aprÃ¨s

```python
import timeit

def benchmark_function(func, *args, iterations=100):
    """Benchmark une fonction"""
    setup = "from __main__ import func, args"
    stmt = "func(*args)"
    
    duration = timeit.timeit(stmt, setup=setup, number=iterations)
    avg = duration / iterations
    
    print(f"{func.__name__}: {avg*1000:.2f}ms per call")
    return avg

# Usage
print("=== BEFORE OPTIMIZATION ===")
before = benchmark_function(old_get_transactions, limit=1000)

print("\n=== AFTER OPTIMIZATION ===")
after = benchmark_function(new_get_transactions, limit=1000)

print(f"\nðŸš€ Speedup: {before/after:.1f}x faster")
```

### 4.2 Tests de charge

```python
# Simuler charge utilisateur
@profile
def simulate_user_session():
    """Simule une session utilisateur complÃ¨te"""
    with measure_block("full_session"):
        # Page Transactions
        with measure_block("load_transactions_page"):
            transactions = load_transactions(limit=50)
        
        # Statistiques
        with measure_block("compute_stats"):
            stats = compute_statistics(transactions)
        
        # Recherche
        with measure_block("search"):
            results = search_transactions("groceries")
    
    return monitor.get_report()

# Lancer 10 sessions
for i in range(10):
    simulate_user_session()

print(monitor.get_report())
```

---

## ðŸŽ¯ Phase 5 : Priorisation

### Matrice Effort/Impact

| Optimisation | Impact | Effort | PrioritÃ© |
|--------------|--------|--------|----------|
| Ajouter JOIN au lieu N+1 | ðŸ”´ Ã‰norme (100x) | ðŸŸ¢ Facile | â­â­â­ |
| CrÃ©er indices DB | ðŸ”´ Ã‰norme (50x) | ðŸŸ¢ Facile | â­â­â­ |
| Cache Streamlit | ðŸŸ¡ Moyen (5x) | ðŸŸ¢ Facile | â­â­ |
| Vectoriser Pandas | ðŸŸ¡ Moyen (10x) | ðŸŸ¡ Moyen | â­â­ |
| Lazy loading | ðŸŸ¢ Petit (2x) | ðŸŸ¡ Moyen | â­ |
| Refactor architecture | ðŸŸ¡ Moyen | ðŸ”´ Difficile | â¸ï¸ |

**RÃ¨gle** : Commencer par les optimisations Haut Impact / Bas Effort

---

## ðŸ“‹ Checklist d'Optimisation

```markdown
### Mesure
- [ ] Ajouter dÃ©corateur @profile sur fonctions clÃ©s
- [ ] Activer monitoring DB queries
- [ ] Afficher rapport performance en sidebar
- [ ] Lancer benchmarks baseline

### Base de DonnÃ©es
- [ ] Identifier requÃªtes N+1
- [ ] Remplacer par JOINs
- [ ] CrÃ©er indices sur colonnes filtrÃ©es
- [ ] Ajouter LIMIT sur toutes les queries
- [ ] Activer WAL mode SQLite

### Pandas
- [ ] Remplacer .iterrows() par vectorisation
- [ ] Utiliser .agg() pour stats groupÃ©es
- [ ] Chunking si DataFrames > 10MB
- [ ] Analyser memory usage

### Streamlit
- [ ] @st.cache_data sur load_*() functions
- [ ] Pagination pour grandes listes
- [ ] Lazy loading des pages
- [ ] DÃ©sactiver auto-rerun inutiles

### Python
- [ ] Sets au lieu de listes pour lookup
- [ ] List comprehensions
- [ ] Dict pour accÃ¨s O(1)
- [ ] Ã‰viter boucles imbriquÃ©es

### Validation
- [ ] Benchmarks avant/aprÃ¨s
- [ ] Tests de charge
- [ ] Documenter gains
- [ ] Monitoring production
```

---

## ðŸŽ“ Ressources

- [Python Profiling](https://docs.python.org/3/library/profile.html)
- [Pandas Performance](https://pandas.pydata.org/docs/user_guide/enhancingperf.html)
- [SQLite Optimization](https://www.sqlite.org/optoverview.html)
- [Streamlit Caching](https://docs.streamlit.io/develop/concepts/architecture/caching)

---

## ðŸš€ RÃ©sultat Attendu

AprÃ¨s optimisation :
- âš¡ **DÃ©marrage** : 8s â†’ 1-2s
- âš¡ **Chargement transactions** : 2s â†’ 0.2s  
- âš¡ **Calcul stats** : 1.5s â†’ 0.1s
- âš¡ **Recherche** : 0.8s â†’ 0.05s

**Total : App 6-10x plus rapide, mÃªme sur WSL !** ðŸŽ¯
